---
title: SLOPE 0.2.0
author: Johan Larsson
date: '2020-04-14'
slug: slope-0-2-0
math: true
categories:
  - R
tags:
  - regularization
  - SLOPE
  - machine-learning
  - generalized linear models
summary: A new version of the SLOPE package has been released with a great
  number of improvements and much new functionality.
lastmod: '2020-04-14T14:55:33+02:00'
featured: true
image:
  caption: 'Directional derivatives of the sorted $\ell_1$ norm.'
  focal_point: 'left'
  preview_only: false
projects: [SLOPE]
bibliography: [../../../static/library.bib]
---

<script src="index_files/header-attrs/header-attrs.js"></script>
<link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections/anchor-sections.js"></script>


<div id="introduction-to-slope" class="section level2">
<h2>Introduction to SLOPE</h2>
<p>SLOPE <span class="citation">(Bogdan et al. 2015)</span> stands for sorted L1 penalized estimation and
is a generalization of OSCAR <span class="citation">(Bondell and Reich 2008)</span>. As the name
suggests, SLOPE
is a type of <span class="math inline">\(\ell_1\)</span>-regularization. More specifically, SLOPE fits
generalized linear models regularized with the sorted <span class="math inline">\(\ell_1\)</span> norm. The
objective in SLOPE is
<span class="math display">\[
\operatorname{minimize}\left\{ f(\beta) + J(\beta \mid \lambda)\right\},
\]</span>
where <span class="math inline">\(f(\beta)\)</span> is typically the log-likelihood of some model in the
family of generalized linear models and
<span class="math inline">\(J(\beta\mid \lambda) = \sum_{i=1}^p \lambda_i|\beta|_{(i)}\)</span> is the
sorted <span class="math inline">\(\ell_1\)</span> norm.</p>
<p>Some people will note that this penalty is a generalization
of the standard <span class="math inline">\(\ell_1\)</span> norm penalty<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. As such,
SLOPE is a type of sparse regression—just like the lasso. Unlike the lasso,
however, SLOPE gracefully handles correlated features.
Whereas the lasso often discards all but a few among a set of
correlated features <span class="citation">(Jia and Yu 2010)</span>,
SLOPE instead <em>clusters</em> such features together by setting such clusters to
have the same coefficient in absolut value.</p>
</div>
<div id="slope-0.2.0" class="section level2">
<h2>SLOPE 0.2.0</h2>
<p>SLOPE 0.2.0 is a new verison of the R package
<a href="https://CRAN.R-project.org/package=SLOPE">SLOPE</a> featuring a range of
improvements over the previous package. If you are completely new to the
package, please start with the <a href="https://jolars.github.io/SLOPE/articles/introduction.html">introductory vignette</a>.</p>
<div id="more-model-families" class="section level3">
<h3>More model families</h3>
<p>Previously, SLOPE only features ordinary least-squares regression. Now the
package features logistic, Poisson, and multinomial regression on top of that.
Just as in other similar packages, this is enabled simply by
setting <code>family = "binomial"</code> for logistic regression, for instance.</p>
<pre class="r"><code>library(SLOPE)
fit &lt;- SLOPE(wine$x, wine$y, family = &quot;multinomial&quot;)</code></pre>
</div>
<div id="regularization-path-fitting" class="section level3">
<h3>Regularization path fitting</h3>
<p>By default, SLOPE now fits a full regularization path instead of
only a single penalty sequence at once. This behavior is now analogous with the
default behavior in glmnet.</p>
<pre class="r"><code>plot(fit)</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="index_files/figure-html/unnamed-chunk-3-1.png" alt="Coefficients from the regularization path for a multinomial model." width="768" />
<p class="caption">
Figure 1: Coefficients from the regularization path for a multinomial model.
</p>
</div>
</div>
<div id="predictor-screening-rules" class="section level3">
<h3>Predictor screening rules</h3>
<p>The package now uses predictor screening rules to vastly improve performance
in the <span class="math inline">\(p \gg n\)</span> domain. Screening rules are part of what makes
other related packages such as glmnet so efficient. In SLOPE, we use a
variant of the strong screening rules for the lasso <span class="citation">(Tibshirani et al. 2012)</span>.</p>
<pre class="r"><code>xy &lt;- SLOPE:::randomProblem(100, 1000)
system.time({SLOPE(xy$x, xy$y, screen = TRUE)})</code></pre>
<pre><code>##    user  system elapsed 
##   3.080   0.000   0.442</code></pre>
<pre class="r"><code>system.time({SLOPE(xy$x, xy$y, screen = FALSE)})</code></pre>
<pre><code>##    user  system elapsed 
##   3.212   0.010   0.424</code></pre>
</div>
<div id="cross-validation-and-caret" class="section level3">
<h3>Cross-validation and caret</h3>
<p>There is now a function <code>trainSLOPE()</code>, which can be used to run
cross-validation for optimal selection of <code>sigma</code> and <code>q</code>. Here, we run
8-fold cross-validation repeated 5 times.</p>
<pre class="r"><code># 8-fold cross-validation repeated 5 times
tune &lt;- trainSLOPE(subset(mtcars, select = c(&quot;mpg&quot;, &quot;drat&quot;, &quot;wt&quot;)),
                   mtcars$hp,
                   q = c(0.1, 0.2),
                   number = 8,
                   repeats = 5)
plot(tune)</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-5"></span>
<img src="index_files/figure-html/unnamed-chunk-5-1.png" alt="Cross-validation with SLOPE." width="672" />
<p class="caption">
Figure 2: Cross-validation with SLOPE.
</p>
</div>
<p>In addition, the package now also features a function <code>caretSLOPE()</code> that
can be used via the excellent caret package, which enables a swath
of resampling methods and comparisons.</p>
</div>
<div id="c-and-admm" class="section level3">
<h3>C++ and ADMM</h3>
<p>All of the performance-critical code for SLOPE has been rewritten in
C++. In addition, the package now features an ADMM solver for
<code>family = "gaussian"</code>, enabled by setting <code>solver = "admm"</code> in the call
to <code>SLOPE()</code>. Preliminary testing shows that this solver is faster for
many designs, particularly when there is high correlation among predictors.</p>
</div>
<div id="sparse-design-matrices" class="section level3">
<h3>Sparse design matrices</h3>
<p>SLOPE now also allows sparse design matrcies of classes from the Matrix package.</p>
</div>
<div id="and-much-more" class="section level3">
<h3>And much more…</h3>
<p>For a full list of changes, please
see <a href="https://jolars.github.io/SLOPE/news/index.html#slope-0-2-0-unreleased">the changelog</a>.</p>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references hanging-indent">
<div id="ref-bogdan2015">
<p>Bogdan, Małgorzata, Ewout van den Berg, Chiara Sabatti, Weijie Su, and Emmanuel J. Candès. 2015. “SLOPE - Adaptive Variable Selection via Convex Optimization.” <em>The Annals of Applied Statistics</em> 9 (3): 1103–40. <a href="https://doi.org/10.1214/15-AOAS842">https://doi.org/10.1214/15-AOAS842</a>.</p>
</div>
<div id="ref-bondell2008">
<p>Bondell, Howard D., and Brian J. Reich. 2008. “Simultaneous Regression Shrinkage, Variable Selection, and Supervised Clustering of Predictors with OSCAR.” <em>Biometrics</em> 64 (1): 115–23. <a href="https://doi.org/10.1111/j.1541-0420.2007.00843.x">https://doi.org/10.1111/j.1541-0420.2007.00843.x</a>.</p>
</div>
<div id="ref-jia2010">
<p>Jia, J., and B. Yu. 2010. “On Model Selection Consistency of the Elastic Net When P <span class="math inline">\(&gt;&gt;\)</span> N.” <em>Statistica Sinica</em> 20 (2): 595–611.</p>
</div>
<div id="ref-tibshirani2012">
<p>Tibshirani, Robert, Jacob Bien, Jerome Friedman, Trevor Hastie, Noah Simon, Jonathan Taylor, and Ryan J. Tibshirani. 2012. “Strong Rules for Discarding Predictors in Lasso-Type Problems.” <em>Journal of the Royal Statistical Society. Series B: Statistical Methodology</em> 74 (2): 245–66. <a href="https://doi.org/10/c4bb85">https://doi.org/10/c4bb85</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Simply set <span class="math inline">\(\lambda_i = \lambda_j\)</span> for
all <span class="math inline">\(i,j \in \{1,\dots,p\}\)</span> and you get the lasso penalty.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
