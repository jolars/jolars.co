@inproceedings{larsson2018,
  title         = {A Case Study in Fitting Area-Proportional {{Euler}} Diagrams with Ellipses Using Eulerr},
  author        = {Larsson, Johan and Gustafsson, Peter},
  booktitle     = {Proceedings of International Workshop on Set Visualization and Reasoning},
  location      = {Edinburgh, United Kingdom},
  publisher     = {CEUR workshop proceedings},
  volume        = {2116},
  pages         = {84--91},
  url           = {http://ceur-ws.org/Vol-2116/paper7.pdf},
  date          = {2018-06-18},
  abstract      = {
    Euler diagrams are common and user-friendly visualizations for set relationships. Most Euler
    diagrams use circles, but circles do not always yield accurate diagrams. A promising
    alternative is ellipses, which, in theory, enable accurate diagrams for a wider range of input.
    Elliptical diagrams, however, have not yet been implemented for more than three sets or
    three-set diagrams where there are disjoint or subset relationships. The aim of this paper is
    to present eulerr: a software package for elliptical Euler diagrams for, in theory, any number
    of sets. It fits Euler diagrams using numerical optimization and exact-area algorithms through
    a two-step procedure, first generating an initial layout using pairwise relationships and then
    finalizing this layout using all set relationships.
  },
  eventtitle    = {Set Visualization and Reasoning 2018}
}

@inproceedings{larsson2020b,
  title         = {The Strong Screening Rule for {{SLOPE}}},
  author        = {Larsson, Johan and Bogdan, Ma\l{}gorzata and Wallin, Jonas},
  booktitle     = {Advances in Neural Information Processing Systems 33},
  location      = {Virtual},
  publisher     = {Curran Associates, Inc.},
  volume        = {33},
  pages         = {14592--14603},
  isbn          = {978-1-71382-954-6},
  url           = {
    https://papers.nips.cc/paper\%5Ffiles/paper/2020/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html
  },
  editor        = {
    Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin,
    Hsuan-Tien
  },
  date          = {2020-12-06/2020-12-12},
  abstract      = {
    Extracting relevant features from data sets where the number of observations n is much smaller
    then the number of predictors p is a major challenge in modern statistics. Sorted L-One
    Penalized Estimation (SLOPE)--a generalization of the lasso---is a promising method within this
    setting. Current numerical procedures for SLOPE, however, lack the efficiency that respective
    tools for the lasso enjoy, particularly in the context of estimating a complete regularization
    path. A key component in the efficiency of the lasso is predictor screening rules: rules that
    allow  predictors to be discarded before estimating the model. This is the first paper to
    establish such a rule for SLOPE. We develop a screening rule for SLOPE by examining its
    subdifferential and show that this rule is a generalization of the strong rule for the lasso.
    Our rule is heuristic, which means that it may discard predictors erroneously. In our paper,
    however, we show that such situations are rare and easily safeguarded against by a simple check
    of the optimality conditions. Our numerical experiments show that the rule performs well in
    practice, leading to improvements by orders of magnitude for data in the \textbackslash (p
    \textbackslash gg n\textbackslash ) domain, as well as incurring no additional computational
    overhead when \$n {$>$} p\$.
  },
  eventtitle    = {34th Conference on Neural Information Processing Systems ({{NeurIPS}} 2020)},
  langid        = {english}
}

@inproceedings{larsson2021,
  title         = {Look-Ahead Screening Rules for the Lasso},
  author        = {Larsson, Johan},
  booktitle     = {22nd {{European}} Young Statisticians Meeting - Proceedings},
  location      = {Athens, Greece},
  publisher     = {{Panteion university of social and political sciences}},
  pages         = {61--65},
  isbn          = {978-960-7943-23-1},
  url           = {https://www.eysm2021.panteion.gr/files/Proceedings\%5FEYSM\%5F2021.pdf},
  editor        = {
    Makridis, Andreas and Milienos, Fotios S. and Papastamoulis, Panagiotis and Parpoula, Christina
    and Rakitzis, Athanasios
  },
  date          = {2021-09-06},
  abstract      = {
    The lasso is a popular method to induce shrinkage and sparsity in the solution vector
    (coefficients) of regression problems, particularly when there are many predictors relative to
    the number of observations. Solving the lasso in this high-dimensional setting can, however, be
    computationally demanding. Fortunately, this demand can be alleviated via the use of screening
    rules that discard predictors prior to fitting the model, leading to a reduced problem to be
    solved. In this paper, we present a new screening strategy: look-ahead screening. Our method
    uses safe screening rules to find a range of penalty values for which a given predictor cannot
    enter the model, thereby screening predictors along the remainder of the path. In experiments
    we show that these look-ahead screening rules outperform the active warm-start version of the
    Gap Safe rules.
  },
  eventtitle    = {22nd {{European}} Young Statisticians Meeting},
  langid        = {english}
}

@inproceedings{larsson2022b,
  title         = {The {{Hessian}} Screening Rule},
  author        = {Larsson, Johan and Wallin, Jonas},
  booktitle     = {Advances in Neural Information Processing Systems 35},
  location      = {New Orleans, USA},
  publisher     = {Curran Associates, Inc.},
  volume        = {35},
  pages         = {15823--15835},
  isbn          = {978-1-71387-108-8},
  url           = {
    https://papers.nips.cc/paper\%5Ffiles/paper/2022/hash/65a925049647eab0aa06a9faf1cd470b-Abstract-Conference.html
  },
  editor        = {
    Koyejo, Sanmi and Mohamed, Sidahmed and Agarwal, Alekh and Belgrave, Danielle and Cho,
    Kyunghyun and Oh, Alice
  },
  date          = {2022-11-28/2022-12-09},
  abstract      = {
    Predictor screening rules, which discard predictors from the design matrix before fitting a
    model, have had considerable impact on the speed with which l1-regularized regression problems,
    such as the lasso, can be solved. Current state-of-the-art screening rules, however, have
    difficulties in dealing with highly-correlated predictors, often becoming too conservative. In
    this paper, we present a new screening rule to deal with this issue: the Hessian Screening
    Rule. The rule uses second-order information from the model to provide more accurate screening
    as well as higher-quality warm starts. The proposed rule outperforms all studied alternatives
    on data sets with high correlation for both l1-regularized least-squares (the lasso) and
    logistic regression. It also performs best overall on the real data sets that we examine.
  },
  eventtitle    = {36th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2022)},
  langid        = {english}
}

@inproceedings{larsson2023,
  title         = {Coordinate Descent for {{SLOPE}}},
  author        = {Larsson, Johan and Klopfenstein, Quentin and Massias, Mathurin and Wallin, Jonas},
  booktitle     = {Proceedings of the 26th International Conference on Artificial Intelligence and Statistics},
  location      = {Valencia, Spain},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  volume        = {206},
  pages         = {4802--4821},
  url           = {https://proceedings.mlr.press/v206/larsson23a.html},
  editor        = {
    Ruiz, Francisco and Dy, Jennifer and family=Meent, given=Jan-Willem, prefix=van de,
    useprefix=true
  },
  date          = {2023-04-25/2023-04-27},
  abstract      = {
    The lasso is the most famous sparse regression and feature selection method. One reason for its
    popularity is the speed at which the underlying optimization problem can be solved. Sorted
    L-One Penalized Estimation (SLOPE) is a generalization of the lasso with appealing statistical
    properties. In spite of this, the method has not yet reached widespread interest. A major
    reason for this is that current software packages that fit SLOPE rely on algorithms that
    perform poorly in high dimensions. To tackle this issue, we propose a new fast algorithm to
    solve the SLOPE optimization problem, which combines proximal gradient descent and proximal
    coordinate descent steps. We provide new results on the directional derivative of the SLOPE
    penalty and its related SLOPE thresholding operator, as well as provide convergence guarantees
    for our proposed solver. In extensive benchmarks on simulated and real data, we demonstrate our
    method's performance against a long list of competing algorithms.
  },
  eventtitle    = {{{AISTATS}} 2023}
}

@inproceedings{moreau2022a,
  title         = {Benchopt: Reproducible, Efficient and Collaborative Optimization Benchmarks},
  shorttitle    = {Benchopt},
  author        = {
    Moreau, Thomas and Massias, Mathurin and Gramfort, Alexandre and Ablin, Pierre and Bannier,
    Pierre-Antoine and Charlier, Benjamin and Dagr\'{e}ou, Mathieu and family=Tour, given=Tom
    Dupr\'{e}, prefix=la, useprefix=false and Durif, Ghislain and Dantas, Cassio F. and
    Klopfenstein, Quentin and Larsson, Johan and Lai, En and Lefort, Tanguy and Mal\'{e}zieux,
    Benoit and Moufad, Badr and Nguyen, Binh T. and Rakotomamonjy, Alain and Ramzi, Zaccharie and
    Salmon, Joseph and Vaiter, Samuel
  },
  booktitle     = {Advances in Neural Information Processing Systems 35},
  location      = {New Orleans, USA},
  publisher     = {Curran Associates, Inc.},
  volume        = {35},
  pages         = {25404--25421},
  isbn          = {978-1-71387-108-8},
  url           = {
    https://proceedings.neurips.cc/paper\%5Ffiles/paper/2022/hash/a30769d9b62c9b94b72e21e0ca73f338-Abstract-Conference.html
  },
  editor        = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  date          = {2022-11-28/2022-12-09},
  abstract      = {
    Numerical validation is at the core of machine learning research as it allows to assess the
    actual impact of new methods, and to confirm the agreement between theory and practice. Yet,
    the rapid development of the field poses several challenges: researchers are confronted with a
    profusion of methods to compare, limited transparency and consensus on best practices, as well
    as tedious re-implementation work. As a result, validation is often very partial, which can
    lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a
    collaborative framework to automate, reproduce and publish optimization benchmarks in machine
    learning across programming languages and hardware architectures. Benchopt simplifies
    benchmarking for the community by providing an off-the-shelf tool for running, sharing and
    extending experiments. To demonstrate its broad usability, we showcase benchmarks on three
    standard learning tasks: \$\textbackslash ell\_2\$-regularized logistic regression, Lasso, and
    ResNet18 training for image classification. These benchmarks highlight key practical findings
    that give a more nuanced view of the state-of-the-art for these problems, showing that for
    practical evaluation, the devil is in the details. We hope that Benchopt will foster
    collaborative work in the community hence improving the reproducibility of research findings.
  },
  eventtitle    = {36th Conference on Neural Information Processing Systems ({{NeurIPS}} 2022)}
}

@online{larsson2025,
  title         = {The Choice of Normalization Influences Shrinkage in Regularized Regression},
  author        = {Larsson, Johan and Wallin, Jonas},
  doi           = {10.48550/arXiv.2501.03821},
  url           = {http://arxiv.org/abs/2501.03821},
  urldate       = {2025-01-08},
  date          = {2025-01-21},
  eprint        = {2501.03821},
  eprinttype    = {arXiv},
  eprintclass   = {stat},
  pubstate      = {prepublished},
  keywords      = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology}
}

@article{larsson2025c,
  title         = {Qualpal: Qualitative Color Palettes for Everyone},
  shorttitle    = {Qualpal},
  author        = {Larsson, Johan},
  volume        = {10},
  number        = {114},
  pages         = {8936},
  doi           = {10.21105/joss.08936},
  issn          = {2475-9066},
  url           = {https://joss.theoj.org/papers/10.21105/joss.08936},
  urldate       = {2025-10-17},
  date          = {2025-10-16},
  journaltitle  = {Journal of Open Source Software},
  langid        = {english}
}
