---
title: "The Strong Screening Rule for SLOPE"
author: 
  - Johan Larsson
  - Małgorzata Bogdan
  - Jonas Wallin
date: 2020-12-06
citation:
  container-title: Advances in Neural Information Processing Systems 33
  editor:
    - Hugo Larochelle
    - Marc'Aurelio Ranzato
    - Raia Hadsell
    - Maria-Florina Balcan
    - Hsuan-Tien Lin
  event-place: Virtual
  event-title: Neural Information Processing Systems 2020
  language: English
  page: 14592–14603
  publisher: Curran Associates, Inc.
  publisher-place: Red Hook, NY, USA
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2020/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html
  volume: '33'
link: https://proceedings.neurips.cc/paper/2020/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html
pdf: https://proceedings.neurips.cc/paper/2020/file/a7d8ae4569120b5bec12e7b6e9648b86-Paper.pdf
github: jolars/slope-screening-code
arxiv: "2005.03730"
categories: 
  - SLOPE
  - Screening Rules
abstract: |
  Extracting relevant features from data sets where the number of observations
  ($n$) is much smaller then the number of predictors ($p$) is a major
  challenge in modern statistics. Sorted L-One Penalized Estimation (SLOPE)—a
  generalization of the lasso—is a promising method within this setting.
  Current numerical procedures for SLOPE, however, lack the efficiency that
  respective tools for the lasso enjoy, particularly in the context of
  estimating a complete regularization path. A key component in the efficiency
  of the lasso is predictor screening rules: rules that allow predictors to be
  discarded before estimating the model. This is the first paper to establish
  such a rule for SLOPE. We develop a screening rule for SLOPE by examining its
  subdifferential and show that this rule is a generalization of the strong
  rule for the lasso. Our rule is heuristic, which means that it may discard
  predictors erroneously. In our paper, however, we show that such situations
  are rare and easily safeguarded against by a simple check of the optimality
  conditions. Our numerical experiments show that the rule performs well in
  practice, leading to improvements by orders of magnitude for data in the ($p
  \gg n$) domain, as well as incurring no additional computational overhead
  when ($n > p$).
---

