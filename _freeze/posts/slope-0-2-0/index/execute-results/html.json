{
  "hash": "e083f5e1da4abc3d4edf54eb61515908",
  "result": {
    "markdown": "---\ntitle: \"SLOPE 0.2.0\"\nauthor: Johan Larsson\ndate: \"2020-04-14\"\ndescription: A new update to the SLOPE package with many exciting features.\ncategories:\n  - r\n  - SLOPE\n  - statistics\nimage: slope.svg\n---\n\n\n\n\n## Introduction to SLOPE\n\nSLOPE [@bogdan2015] stands for sorted L1 penalized estimation and\nis a generalization of OSCAR [@bondell2008]. As the name \nsuggests, SLOPE\nis a type of $\\ell_1$-regularization. More specifically, SLOPE fits \ngeneralized linear models regularized with the sorted $\\ell_1$ norm. The\nobjective in SLOPE is\n\n$$\n\\operatorname{minimize}\\left\\{ f(\\beta) + J(\\beta \\mid \\lambda)\\right\\},\n$$\n\nwhere $f(\\beta)$ is typically the log-likelihood of some model in the \nfamily of generalized linear models and \n\n$$J(\\beta\\mid \\lambda) = \\sum_{i=1}^p \\lambda_i|\\beta|_{(i)}$$\n\nis the\nsorted $\\ell_1$ norm.\n\nSome people will note that this penalty is a generalization\nof the standard $\\ell_1$ norm penalty[^1]. As such,\nSLOPE is a type of sparse regression---just like the lasso. Unlike the lasso,\nhowever, SLOPE gracefully handles correlated features.\nWhereas the lasso often discards all but a few among a set of \ncorrelated features [@jia2010], \nSLOPE instead *clusters* such features together by setting such clusters to\nhave the same coefficient in absolut value.\n\n[^1]: Simply set $\\lambda_i = \\lambda_j$ for all $i,j \\in \\{1,\\dots,p\\}$ and you get the lasso penalty.\n\n## SLOPE 0.2.0\n\nSLOPE 0.2.0 is a new verison of the R package\n[SLOPE](https://CRAN.R-project.org/package=SLOPE) featuring a range of\nimprovements over the previous package. If you are completely new to the \npackage, please start with the [introductory vignette](https://jolars.github.io/SLOPE/articles/introduction.html).\n\n### More model families\n\nPreviously, SLOPE only features ordinary least-squares regression. Now the\npackage features logistic, Poisson, and multinomial regression on top of that.\nJust as in other similar packages, this is enabled simply by\nsetting `family = \"binomial\"` for logistic regression, for instance.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(SLOPE)\nfit <- SLOPE(wine$x, wine$y, family = \"multinomial\")\n```\n:::\n\n\n### Regularization path fitting\n\nBy default, SLOPE now fits a full regularization path instead of\nonly a single penalty sequence at once. This behavior is now analogous with the \ndefault behavior in glmnet.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(fit)\n```\n\n::: {.cell-output-display}\n![Coefficients from the regularization path for a multinomial model.](index_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n### Predictor screening rules\n\nThe package now uses predictor screening rules to vastly improve performance\nin the $p \\gg n$ domain. Screening rules are part of what makes\nother related packages such as glmnet so efficient. In SLOPE, we use a\nvariant of the strong screening rules for the lasso [@tibshirani2012].\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxy <- SLOPE:::randomProblem(100, 1000)\nsystem.time({SLOPE(xy$x, xy$y, screen = TRUE)})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  1.296   0.000   0.168 \n```\n:::\n\n```{.r .cell-code}\nsystem.time({SLOPE(xy$x, xy$y, screen = FALSE)})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  3.639   0.007   0.463 \n```\n:::\n:::\n\n\n### Cross-validation and caret\n\nThere is now a function `trainSLOPE()`, which can be used to run\ncross-validation for optimal selection of `sigma` and `q`. Here, we run\n8-fold cross-validation repeated 5 times.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 8-fold cross-validation repeated 5 times\ntune <- trainSLOPE(\n  subset(mtcars, select = c(\"mpg\", \"drat\", \"wt\")),\n  mtcars$hp,\n  q = c(0.1, 0.2),\n  number = 8,\n  repeats = 5\n)\nplot(tune)\n```\n\n::: {.cell-output-display}\n![Cross-validation with SLOPE.](index_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nIn addition, the package now also features a function `caretSLOPE()` that\ncan be used via the excellent caret package, which enables a swath\nof resampling methods and comparisons.\n\n### C++ and ADMM\n\nAll of the performance-critical code for SLOPE has been rewritten in \nC++. In addition, the package now features an ADMM solver for\n`family = \"gaussian\"`, enabled by setting `solver = \"admm\"` in the call\nto `SLOPE()`. Preliminary testing shows that this solver is faster for\nmany designs, particularly when there is high correlation among predictors.\n\n### Sparse design matrices\n\nSLOPE now also allows sparse design matrcies of classes from the Matrix package.\n\n### And much more...\n\nFor a full list of changes, please\nsee [the changelog](https://jolars.github.io/SLOPE/news/index.html#slope-0-2-0-unreleased).\n\n## References\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}