---
title: "SLOPE 0.2.0"
categories:
  - R
tags:
  - regularization
  - SLOPE
  - machine-learning
  - generalized linear models
  - statistics
excerpt: A new version of the SLOPE package has been released with a great
  number of improvements and much new functionality.
---



## Introduction to SLOPE

SLOPE {% cite bogdan2015 %} stands for sorted L1 penalized estimation and
is a generalization of OSCAR {% cite bondell2008 %}. As the name 
suggests, SLOPE
is a type of $\ell_1$-regularization. More specifically, SLOPE fits 
generalized linear models regularized with the sorted $\ell_1$ norm. The
objective in SLOPE is

$$
\operatorname{minimize}\left\{ f(\beta) + J(\beta \mid \lambda)\right\},
$$

where $f(\beta)$ is typically the log-likelihood of some model in the 
family of generalized linear models and 

$$J(\beta\mid \lambda) = \sum_{i=1}^p \lambda_i|\beta|_{(i)}$$

is the
sorted $\ell_1$ norm.

Some people will note that this penalty is a generalization
of the standard $\ell_1$ norm penalty[^1]. As such,
SLOPE is a type of sparse regression---just like the lasso. Unlike the lasso,
however, SLOPE gracefully handles correlated features.
Whereas the lasso often discards all but a few among a set of 
correlated features {% cite jia2010 %}, 
SLOPE instead *clusters* such features together by setting such clusters to
have the same coefficient in absolut value.

[^1]: Simply set $\lambda_i = \lambda_j$ for all $i,j \in \{1,\dots,p\}$ and you get the lasso penalty.

## SLOPE 0.2.0

SLOPE 0.2.0 is a new verison of the R package
[SLOPE](https://CRAN.R-project.org/package=SLOPE) featuring a range of
improvements over the previous package. If you are completely new to the 
package, please start with the [introductory vignette](https://jolars.github.io/SLOPE/articles/introduction.html).

### More model families

Previously, SLOPE only features ordinary least-squares regression. Now the
package features logistic, Poisson, and multinomial regression on top of that.
Just as in other similar packages, this is enabled simply by
setting `family = "binomial"` for logistic regression, for instance.


```r
library(SLOPE)
fit <- SLOPE(wine$x, wine$y, family = "multinomial")
```

### Regularization path fitting

By default, SLOPE now fits a full regularization path instead of
only a single penalty sequence at once. This behavior is now analogous with the 
default behavior in glmnet.


```r
plot(fit)
```

<figure class="align-center" style="max-width: 768px">
<img src="/2020-04-14-slope-0.2.0_files/figure-html/unnamed-chunk-3-1.png" alt="Coefficients from the regularization path for a multinomial model." width="768" />
<figcaption>Coefficients from the regularization path for a multinomial model.</figcaption>
</figure>

### Predictor screening rules

The package now uses predictor screening rules to vastly improve performance
in the $p \gg n$ domain. Screening rules are part of what makes
other related packages such as glmnet so efficient. In SLOPE, we use a
variant of the strong screening rules for the lasso {% cite tibshirani2012 %}.


```r
xy <- SLOPE:::randomProblem(100, 1000)
system.time({SLOPE(xy$x, xy$y, screen = TRUE)})
```

```
##    user  system elapsed 
##  30.240   0.143   5.349
```

```r
system.time({SLOPE(xy$x, xy$y, screen = FALSE)})
```

```
##    user  system elapsed 
##  11.147   0.078   1.650
```

### Cross-validation and caret

There is now a function `trainSLOPE()`, which can be used to run
cross-validation for optimal selection of `sigma` and `q`. Here, we run
8-fold cross-validation repeated 5 times.


```r
# 8-fold cross-validation repeated 5 times
tune <- trainSLOPE(subset(mtcars, select = c("mpg", "drat", "wt")),
                   mtcars$hp,
                   q = c(0.1, 0.2),
                   number = 8,
                   repeats = 5)
plot(tune)
```

<figure class="align-center" style="max-width: 672px">
<img src="/2020-04-14-slope-0.2.0_files/figure-html/unnamed-chunk-5-1.png" alt="Cross-validation with SLOPE." width="672" />
<figcaption>Cross-validation with SLOPE.</figcaption>
</figure>

In addition, the package now also features a function `caretSLOPE()` that
can be used via the excellent caret package, which enables a swath
of resampling methods and comparisons.

### C++ and ADMM

All of the performance-critical code for SLOPE has been rewritten in 
C++. In addition, the package now features an ADMM solver for
`family = "gaussian"`, enabled by setting `solver = "admm"` in the call
to `SLOPE()`. Preliminary testing shows that this solver is faster for
many designs, particularly when there is high correlation among predictors.

### Sparse design matrices

SLOPE now also allows sparse design matrcies of classes from the Matrix package.

### And much more...

For a full list of changes, please
see [the changelog](https://jolars.github.io/SLOPE/news/index.html#slope-0-2-0-unreleased).

## References
